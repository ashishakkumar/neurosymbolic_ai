{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "\n",
    "class Neo4jLoader:\n",
    "    def __init__(self, uri: str, username: str, password: str):\n",
    "        \"\"\"Initialize Neo4j connection\"\"\"\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the driver connection\"\"\"\n",
    "        self.driver.close()\n",
    "\n",
    "    def load_data(self, json_file_path: str):\n",
    "        \"\"\"Load data from JSON file and import to Neo4j\"\"\"\n",
    "        try:\n",
    "            with open(json_file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error reading JSON file: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Create constraints for unique IDs\n",
    "        self._create_constraints()\n",
    "\n",
    "        # Load data in batches\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i + batch_size]\n",
    "            self._process_batch(batch)\n",
    "            self.logger.info(f\"Processed {i + len(batch)} records\")\n",
    "\n",
    "    def _create_constraints(self):\n",
    "        \"\"\"Create necessary constraints\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            try:\n",
    "                session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (d:Disease) REQUIRE d.id IS UNIQUE\")\n",
    "                session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (s:Symptom) REQUIRE s.cui IS UNIQUE\")\n",
    "                self.logger.info(\"Created constraints successfully\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error creating constraints: {e}\")\n",
    "                raise\n",
    "\n",
    "    def _process_batch(self, batch: List[Dict]):\n",
    "        \"\"\"Process a batch of disease records\"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $batch AS disease\n",
    "        MERGE (d:Disease {id: disease.disease_id})\n",
    "        ON CREATE SET d.name = disease.disease_name\n",
    "        WITH d, disease.symptoms AS symptoms\n",
    "        UNWIND CASE WHEN symptoms IS NOT NULL AND size(symptoms) > 0 THEN symptoms ELSE [null] END AS symptom\n",
    "        WITH d, symptom\n",
    "        WHERE symptom IS NOT NULL\n",
    "        MERGE (s:Symptom {cui: symptom.symptom_cui})\n",
    "        ON CREATE SET s.name = symptom.synonyms[0],\n",
    "                     s.synonyms = symptom.synonyms\n",
    "        ON MATCH SET s.synonyms = \n",
    "            CASE \n",
    "                WHEN s.synonyms IS NULL THEN symptom.synonyms\n",
    "                ELSE s.synonyms + [x IN symptom.synonyms WHERE NOT x IN s.synonyms]\n",
    "            END\n",
    "        MERGE (d)-[r:HAS_SYMPTOM]->(s)\n",
    "        ON CREATE SET r.weight = symptom.positive_count\n",
    "        ON MATCH SET r.weight = r.weight + symptom.positive_count\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            try:\n",
    "                session.run(query, batch=batch)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing batch: {e}\")\n",
    "                raise\n",
    "\n",
    "    def verify_data(self):\n",
    "        \"\"\"Verify loaded data with some basic statistics\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            disease_count = session.run(\"MATCH (d:Disease) RETURN COUNT(d) as count\").single()[\"count\"]\n",
    "            symptom_count = session.run(\"MATCH (s:Symptom) RETURN COUNT(s) as count\").single()[\"count\"]\n",
    "            rel_count = session.run(\"MATCH ()-[r:HAS_SYMPTOM]->() RETURN COUNT(r) as count\").single()[\"count\"]\n",
    "            \n",
    "            return {\n",
    "                \"diseases\": disease_count,\n",
    "                \"symptoms\": symptom_count,\n",
    "                \"relationships\": rel_count\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connection parameters - Note the new port 6687 for bolt\n",
    "uri = \"bolt://localhost:8687\"\n",
    "username = \"neo4j\"\n",
    "password = \"neo4j_pass5\"  # Replace with your password\n",
    "json_file_path = \"sparse_kg_data.json\"  # Replace with your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:neo4j.io:Failed to write data to connection IPv4Address(('localhost', 8687)) (ResolvedIPv6Address(('::1', 8687, 0, 0)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    # Initialize loader\n",
    "loader = Neo4jLoader(uri, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created constraints successfully\n",
      "INFO:__main__:Processed 100 records\n",
      "INFO:__main__:Processed 200 records\n",
      "INFO:__main__:Processed 300 records\n",
      "INFO:__main__:Processed 400 records\n",
      "INFO:__main__:Processed 500 records\n",
      "INFO:__main__:Processed 600 records\n",
      "INFO:__main__:Processed 700 records\n",
      "INFO:__main__:Processed 800 records\n",
      "INFO:__main__:Processed 900 records\n",
      "INFO:__main__:Processed 1000 records\n",
      "INFO:__main__:Processed 1100 records\n",
      "INFO:__main__:Processed 1200 records\n",
      "INFO:__main__:Processed 1300 records\n",
      "INFO:__main__:Processed 1400 records\n",
      "INFO:__main__:Processed 1500 records\n",
      "INFO:__main__:Processed 1600 records\n",
      "INFO:__main__:Processed 1700 records\n",
      "INFO:__main__:Processed 1800 records\n",
      "INFO:__main__:Processed 1900 records\n",
      "INFO:__main__:Processed 2000 records\n",
      "INFO:__main__:Processed 2100 records\n",
      "INFO:__main__:Processed 2200 records\n",
      "INFO:__main__:Processed 2300 records\n",
      "INFO:__main__:Processed 2400 records\n",
      "INFO:__main__:Processed 2500 records\n",
      "INFO:__main__:Processed 2600 records\n",
      "INFO:__main__:Processed 2700 records\n",
      "INFO:__main__:Processed 2800 records\n",
      "INFO:__main__:Processed 2900 records\n",
      "INFO:__main__:Processed 3000 records\n",
      "INFO:__main__:Processed 3100 records\n",
      "INFO:__main__:Processed 3200 records\n",
      "INFO:__main__:Processed 3300 records\n",
      "INFO:__main__:Processed 3400 records\n",
      "INFO:__main__:Processed 3500 records\n",
      "INFO:__main__:Processed 3600 records\n",
      "INFO:__main__:Processed 3700 records\n",
      "INFO:__main__:Processed 3800 records\n",
      "INFO:__main__:Processed 3900 records\n",
      "INFO:__main__:Processed 4000 records\n",
      "INFO:__main__:Processed 4100 records\n",
      "INFO:__main__:Processed 4200 records\n",
      "INFO:__main__:Processed 4300 records\n",
      "INFO:__main__:Processed 4400 records\n",
      "INFO:__main__:Processed 4500 records\n",
      "INFO:__main__:Processed 4600 records\n",
      "INFO:__main__:Processed 4700 records\n",
      "INFO:__main__:Processed 4800 records\n",
      "INFO:__main__:Processed 4900 records\n",
      "INFO:__main__:Processed 5000 records\n",
      "INFO:__main__:Processed 5100 records\n",
      "INFO:__main__:Processed 5200 records\n",
      "INFO:__main__:Processed 5300 records\n",
      "INFO:__main__:Processed 5400 records\n",
      "INFO:__main__:Processed 5500 records\n",
      "INFO:__main__:Processed 5600 records\n",
      "INFO:__main__:Processed 5700 records\n",
      "INFO:__main__:Processed 5800 records\n",
      "INFO:__main__:Processed 5900 records\n",
      "INFO:__main__:Processed 6000 records\n",
      "INFO:__main__:Processed 6100 records\n",
      "INFO:__main__:Processed 6200 records\n",
      "INFO:__main__:Processed 6300 records\n",
      "INFO:__main__:Processed 6400 records\n",
      "INFO:__main__:Processed 6500 records\n",
      "INFO:__main__:Processed 6600 records\n",
      "INFO:__main__:Processed 6700 records\n",
      "INFO:__main__:Processed 6800 records\n",
      "INFO:__main__:Processed 6900 records\n",
      "INFO:__main__:Processed 7000 records\n",
      "INFO:__main__:Processed 7100 records\n",
      "INFO:__main__:Processed 7200 records\n",
      "INFO:__main__:Processed 7300 records\n",
      "INFO:__main__:Processed 7400 records\n",
      "INFO:__main__:Processed 7500 records\n",
      "INFO:__main__:Processed 7600 records\n",
      "INFO:__main__:Processed 7700 records\n",
      "INFO:__main__:Processed 7800 records\n",
      "INFO:__main__:Processed 7900 records\n",
      "INFO:__main__:Processed 8000 records\n",
      "INFO:__main__:Processed 8100 records\n",
      "INFO:__main__:Processed 8200 records\n",
      "INFO:__main__:Processed 8300 records\n",
      "INFO:__main__:Processed 8400 records\n",
      "INFO:__main__:Processed 8500 records\n",
      "INFO:__main__:Processed 8600 records\n",
      "INFO:__main__:Processed 8700 records\n",
      "INFO:__main__:Processed 8800 records\n",
      "INFO:__main__:Processed 8900 records\n",
      "INFO:__main__:Processed 9000 records\n",
      "INFO:__main__:Processed 9100 records\n",
      "INFO:__main__:Processed 9200 records\n",
      "INFO:__main__:Processed 9300 records\n",
      "INFO:__main__:Processed 9400 records\n",
      "INFO:__main__:Processed 9500 records\n",
      "INFO:__main__:Processed 9600 records\n",
      "INFO:__main__:Processed 9700 records\n",
      "INFO:__main__:Processed 9800 records\n",
      "INFO:__main__:Processed 9900 records\n",
      "INFO:__main__:Processed 10000 records\n",
      "INFO:__main__:Processed 10100 records\n",
      "INFO:__main__:Processed 10200 records\n",
      "INFO:__main__:Processed 10300 records\n",
      "INFO:__main__:Processed 10400 records\n",
      "INFO:__main__:Processed 10500 records\n",
      "INFO:__main__:Processed 10600 records\n",
      "INFO:__main__:Processed 10700 records\n",
      "INFO:__main__:Processed 10800 records\n",
      "INFO:__main__:Processed 10900 records\n",
      "INFO:__main__:Processed 11000 records\n",
      "INFO:__main__:Processed 11100 records\n",
      "INFO:__main__:Processed 11200 records\n",
      "INFO:__main__:Processed 11300 records\n",
      "INFO:__main__:Processed 11400 records\n",
      "INFO:__main__:Processed 11500 records\n",
      "INFO:__main__:Processed 11600 records\n",
      "INFO:__main__:Processed 11700 records\n",
      "INFO:__main__:Processed 11800 records\n",
      "INFO:__main__:Processed 11900 records\n",
      "INFO:__main__:Processed 12000 records\n",
      "INFO:__main__:Processed 12100 records\n",
      "INFO:__main__:Processed 12200 records\n",
      "INFO:__main__:Processed 12300 records\n",
      "INFO:__main__:Processed 12400 records\n",
      "INFO:__main__:Processed 12500 records\n",
      "INFO:__main__:Processed 12600 records\n",
      "INFO:__main__:Processed 12700 records\n",
      "INFO:__main__:Processed 12800 records\n",
      "INFO:__main__:Processed 12900 records\n",
      "INFO:__main__:Processed 13000 records\n",
      "INFO:__main__:Processed 13100 records\n",
      "INFO:__main__:Processed 13145 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader.load_data(json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed successfully!\n",
      "Statistics:\n",
      "{\n",
      "  \"diseases\": 13145,\n",
      "  \"symptoms\": 1478,\n",
      "  \"relationships\": 21363\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify data\n",
    "stats = loader.verify_data()\n",
    "print(\"Data loading completed successfully!\")\n",
    "print(f\"Statistics:\\n{json.dumps(stats, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"neo4j://localhost:8687\" \n",
    "auth = (\"neo4j\", \"neo4j_pass5\")\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=auth) \n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "            session.run(\"CREATE FULLTEXT INDEX symptomIndex FOR (s:Symptom) ON EACH [s.name]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
